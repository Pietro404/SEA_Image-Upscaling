Descrizione dell’algoritmo bicubico

L’interpolazione bicubica calcola il valore di ciascun pixel di output come combinazione pesata di un intorno 4×4 di pixel dell’immagine di input.
Nel caso in esame è stato utilizzato un kernel bicubico di tipo Catmull–Rom spline (parametro a = −0.5), che garantisce continuità delle derivate prime e buone proprietà di interpolazione visiva.

Per ogni pixel di output vengono eseguite:
16 letture di pixel sorgente
16 moltiplicazioni pixel × peso
accumuli orizzontali e verticali
normalizzazione e clamp finale nel range [0, 255]
L’algoritmo è intrinsecamente più costoso rispetto a nearest neighbour e bilinear, risultando principalmente compute-bound.

3. Implementazioni considerate
3.1 Versione CPU v1 – Implementazione naïf

La versione iniziale implementa direttamente la formula bicubica:
doppio ciclo su pixel di output
calcolo dinamico dei pesi bicubici
gestione dei bordi tramite std::clamp
uso estensivo di floating point
Questa versione presenta numerosi ricalcoli e branch all’interno dei loop più interni, risultando poco efficiente.

3.2 Versione CPU v2 – Ottimizzazione con tiling e precalcolo parziale

In questa versione sono state introdotte:
elaborazione a blocchi (tiling 16×16)
precalcolo dei pesi verticali per riga
riduzione delle operazioni ripetute
Pur migliorando le prestazioni, la gestione dinamica dei bordi e l’uso estensivo di floating point limitano il guadagno ottenibile.

3.3 Versione CPU v3 – Ottimizzazione della memoria e del flusso dati

Questa versione rappresenta la migliore implementazione seriale:
padding esplicito dell’immagine di input (eliminazione dei clamp nel kernel)
precalcolo completo dei pesi orizzontali
uso di puntatori a righe contigue
accesso RGB contiguo
unrolling manuale dei loop interni
rimozione della normalizzazione (somma dei pesi ≈ 1)
clamp branchless
Queste ottimizzazioni trasformano il kernel in una versione altamente efficiente e prevedibile per la CPU.

3.4 Versione CPU v4 – Variante senza padding

In questa versione il padding viene rimosso e la gestione dei bordi viene reinserita tramite min/max.
Nonostante il mantenimento del tiling, la reintroduzione di branch nel loop interno comporta una riduzione delle prestazioni rispetto alla versione v3.

3.5 Versione OpenMP

La versione OpenMP parallelizza il lavoro sui blocchi dell’immagine:
utilizzo di #pragma omp parallel for collapse(2)
suddivisione del carico su più core
assenza di dipendenze tra thread
Questa versione sfrutta efficacemente il parallelismo a livello di thread, ottenendo uno speedup quasi lineare.

3.6 Versioni SIMD SSE2
Sono state implementate due versioni SIMD basate su SSE2:
utilizzo di lookup table (LUT) in fixed-point Q7 per i pesi bicubici
eliminazione del floating point
uso di _mm_madd_epi16 per moltiplicazioni e accumuli
gestione accurata del range numerico per evitare overflow
La seconda versione SSE2 introduce un padding più ampio dell’immagine di input, eliminando completamente i controlli sui bordi e consentendo carichi SIMD lineari e prevedibili.

4. Risultati sperimentali
Implementazione	Tempo (ms)	Speedup
CPU v1	~3921	1×
CPU v2	~1616	2.4×
CPU v3	~743	5.3×
CPU v4	~957	4.1×
CPU OpenMP	~198	~20×
CPU SSE2	~481	~8×
CPU SSE2 v2	~189	~21×
5. Analisi delle prestazioni

I risultati mostrano chiaramente che:
la versione naïf è dominata dal costo computazionale e dai branch
le ottimizzazioni di memoria e di flusso dati hanno un impatto maggiore delle sole ottimizzazioni matematiche
l’eliminazione dei controlli sui bordi nel kernel è cruciale
l’interpolazione bicubica risulta compute-bound
OpenMP e SIMD permettono di sfruttare efficacemente il parallelismo della CPU
una singola versione SIMD ben ottimizzata può raggiungere prestazioni paragonabili a una versione multithread

6. Conclusioni

L’esperienza dimostra come un’implementazione diretta di un algoritmo di interpolazione bicubica possa risultare estremamente inefficiente se non si considerano gli aspetti architetturali della CPU.
Attraverso ottimizzazioni progressive, è stato possibile ottenere uno speedup superiore a 20× rispetto alla versione naïf, sfruttando sia il parallelismo a livello di thread (OpenMP) sia il parallelismo a livello di dati (SIMD SSE2).
Il lavoro evidenzia l’importanza di:
ridurre branch e ricalcoli nei loop interni
migliorare la località dei dati in memoria
scegliere il modello di parallelismo più adatto al tipo di carico computazionale
